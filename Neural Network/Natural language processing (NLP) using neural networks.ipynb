{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) using neural networks has seen significant advancements in recent years. Neural networks, particularly deep learning models, have revolutionized the field of NLP by enabling more accurate and efficient processing of human language. Here's a brief overview of how neural networks are used in NLP:\n",
    "\n",
    "    Word Embeddings: Neural networks are used to learn distributed representations of words, known as word embeddings. Techniques like Word2Vec, GloVe, and FastText utilize neural networks to generate dense vector representations of words, capturing semantic similarities and relationships between them.\n",
    "\n",
    "    Sequence Modeling: Recurrent Neural Networks (RNNs) and their variants like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) are commonly used for sequence modeling tasks in NLP. They can effectively capture dependencies between words in a sequence, making them suitable for tasks like language modeling, sequence-to-sequence learning, and text generation.\n",
    "\n",
    "    Convolutional Neural Networks (CNNs): While initially developed for computer vision tasks, CNNs have also been applied to NLP tasks, particularly for tasks involving fixed-size inputs such as text classification and sentiment analysis. CNNs can capture local patterns and features within sequences of text.\n",
    "\n",
    "    Attention Mechanisms: Attention mechanisms, popularized by models like the Transformer, have become fundamental in various NLP tasks. Attention allows models to focus on relevant parts of the input sequence, improving performance in tasks like machine translation, text summarization, and question answering.\n",
    "\n",
    "    Pre-trained Language Models: Pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and their variants have gained widespread popularity. These models are trained on large corpora of text and then fine-tuned on specific tasks. They achieve state-of-the-art performance across a range of NLP tasks, including text classification, named entity recognition, and sentiment analysis.\n",
    "\n",
    "    Transfer Learning: Transfer learning, leveraging pre-trained models, has become a dominant paradigm in NLP. Instead of training models from scratch, practitioners fine-tune pre-trained models on domain-specific or task-specific data, significantly reducing the amount of labeled data required and improving performance.\n",
    "\n",
    "    Multi-task Learning: Neural networks allow for multi-task learning, where a single model is trained to perform multiple NLP tasks simultaneously. This approach can lead to better generalization and resource utilization.\n",
    "\n",
    "Overall, neural networks have revolutionized natural language processing, enabling the development of sophisticated models that can understand, generate, and manipulate human language with remarkable accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     47\u001b[0m test_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural language processing is fascinating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Neural networks are amazing tools for NLP\",\n",
    "    \"Text classification using deep learning is fascinating\"\n",
    "]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert text to sequences and pad sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, [1, 1, 0], epochs=10, batch_size=1)\n",
    "\n",
    "# Predict\n",
    "test_texts = [\n",
    "    \"Natural language processing is fascinating\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def prepare_data(texts, labels, max_sequence_length=None):\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Convert text to sequences and pad sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if not max_sequence_length:\n",
    "        max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    return padded_sequences, labels, tokenizer, max_sequence_length\n",
    "\n",
    "def build_model(input_dim, output_dim, max_sequence_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),  # Pooling layer to reduce dimensions\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Sample text data and labels\n",
    "texts = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Neural networks are amazing tools for NLP\",\n",
    "    \"Text classification using deep learning is fascinating\"\n",
    "]\n",
    "labels = [1, 1, 0]  # Example binary labels\n",
    "\n",
    "# Prepare data\n",
    "X, y, tokenizer, max_sequence_length = prepare_data(texts, labels)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(len(tokenizer.word_index) + 1, 1, max_sequence_length)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "# Predict\n",
    "test_texts = [\n",
    "    \"Natural language processing is fascinating\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     47\u001b[0m test_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural language processing is fascinating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def prepare_data(texts, labels, max_sequence_length=None):\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Convert text to sequences and pad sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if not max_sequence_length:\n",
    "        max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    return padded_sequences, labels, tokenizer, max_sequence_length\n",
    "\n",
    "def build_model(input_dim, output_dim, max_sequence_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),  # Pooling layer to reduce dimensions\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Sample text data and labels\n",
    "texts = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Neural networks are amazing tools for NLP\",\n",
    "    \"Text classification using deep learning is fascinating\"\n",
    "]\n",
    "labels = [1, 1, 0]  # Example binary labels\n",
    "\n",
    "# Prepare data\n",
    "X, y, tokenizer, max_sequence_length = prepare_data(texts, labels)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(len(tokenizer.word_index) + 1, 1, max_sequence_length)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "# Predict\n",
    "test_texts = [\n",
    "    \"Natural language processing is fascinating\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 18ms/step - loss: 0.6884 - accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5764 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5566 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "[[0.553547]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def prepare_data(texts, labels, max_sequence_length=None):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    if not max_sequence_length:\n",
    "        max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    return padded_sequences, np.array(labels), tokenizer, max_sequence_length  # Convert labels to numpy array\n",
    "\n",
    "def build_model(input_dim, output_dim, max_sequence_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),  \n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Sample text data and labels\n",
    "texts = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Neural networks are amazing tools for NLP\",\n",
    "    \"Text classification using deep learning is fascinating\"\n",
    "]\n",
    "labels = [1, 1, 0]  # Example binary labels\n",
    "\n",
    "# Prepare data\n",
    "X, y, tokenizer, max_sequence_length = prepare_data(texts, labels)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(len(tokenizer.word_index) + 1, 1, max_sequence_length)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "# Predict\n",
    "test_texts = [\n",
    "    \"Natural language processing is fascinating\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "predictions = model.predict(test_padded_sequences)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
